{
    "train": [
        {
            "task": "rev_util",
            "aspect": "verifiability",
            "labels": 3,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The claims in your comment should be justified or proved by providing logical reasoning, using common sense, or referencing external sources.\n\n\n[CRITERIA]: Claim justification-verification can be done either by logical reasoning supporting the claim, common sense knowledge in the field verifying the claim (e.g., referencing established practices or standards), or external references substantiating the claim. Scoring rubric is as follows:\n1: The comment contains a claim without any supporting evidence or justification.\n2: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\n3: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\n4: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\n5: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through: - Clear and precise reasoning or explanation. - Specific and relevant references to external works or data. - Logical and unassailable common-sense arguments.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: A number of claims from this paper would benefit from more in-depth analysis.\n\nEVALUATION:\n\n<reasoning>The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n\nEVALUATION:\n\n<reasoning>The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable. Therefore, evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n\nEVALUATION:\n\n<reasoning>The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at the evaluation score 3.\"</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n\nEVALUATION:\n\n<reasoning>The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n\nEVALUATION:\n\n<reasoning>The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is fully verifiable and should get the evaluation score 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: - Data collection and annotation are not clear - The Enforceable Annotation might have ethical issues. What will be the reward for the 10 law experts? why did they volunteer? Does it count toward their study (credit), or will they co-author the paper? This is a serious issue, which might lead to the low quality of the data.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "positioning_check",
            "labels": 0,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Your task is to write a related work section paragraph for a scientific paper that states the paper's contribution or its position among the literature in each paragraph.\n\n\n[CRITERIA]: The generated paragraph should explicitly or implicitly mention the main paper's contribution or position among existing literature. Ensure that contribution and/or positioning statements should be aligned with the specific focus of the paragraph. Scoring rubric is as follows:\n0: The answer paragraph fails to explicitly or implicitly mention the main paper's contribution or position among existing literature.\n1: The answer paragraph explicitly or implicitly mention the main paper's contribution or position among existing literature.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios. Part of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nEVALUATION:\n\n<reasoning>The draft states the main paper's contribution and how it differs from existing literature. It outlines the limitations of previous studies and then explicitly states how the current work addresses these issues through specific contributions, such as manual dataset construction and handling cold start scenarios without pre-defined TDM triples. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement. In addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\n\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nEVALUATION:\n\n<reasoning>The draft provides a comprehensive overview of related work but does not explicitly or implicitly mention the main paper's own contribution or how it differs from the existing literature. It talks about the efforts made but doesn't bridge into how this work builds upon them or addresses gaps they left. It focuses solely on summarizing prior work without establishing the unique position or advancement made by the current study. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 2>\n\n\n[ANSWER]: A number of recent studies have investigated conformal prediction (CP) under various forms of distribution shift.  Works such as those by Vovk\u202fet\u202fal. and subsequent extensions have formalized weighted exchangeability, allowing CP to retain finite\u2011sample coverage guarantees when the test distribution differs from the training distribution.  In the context of causal inference, CP has been applied to counterfactual settings, where the problem is cast as a covariate\u2011shift scenario and predictive intervals are constructed for treatment\u2011and\u2011control outcomes under specific assumptions about the shift mechanism."
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "grounding_specificity",
            "labels": 3,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should refer to a specific part of the paper and clearly identify the issue with that part.\n\n\n[CRITERIA]: For fully grounded comment, the author can accurately pinpoint the section, table, figure, or unique aspect being addressed. For weak grounded comment, the author can make an educated guess but cannot precisely identify the referenced part. For specificity, the comment should detail what is wrong or missing in the referenced part. If external work is mentioned, it should also provide specific examples. Scoring rubric is as follows:\n1: The comment is not grounded at all. It does not identify a specific area in the paper. The comment is highly unspecific.\n2: The authors cannot confidently determine which part the comment addresses. Further, the comment does not specify what needs to be addressed in this part.\n3: The authors cannot confidently determine which part the comment addresses. However, the comment clearly specifies what needs to be addressed in this part.\n4: The comment explicitly mentions which part of the paper it addresses, or it should be obvious to the authors. However, this comment does not specify what needs to be addressed in this part.\n5: The comment explicitly mentions which part of the paper it addresses, and it is obvious to the authors. The comment specifies what needs to be addressed in this part.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: The writing should be improved. Some points in the paper is unclear to me.\n\nEVALUATION:\n\n<reasoning>The comment is not grounded as it does not identify any specific part of the paper that needs improvement. It generally mentions that the writing should be improved and that some points are unclear, but it does not specify which points or sections are problematic. This lack of detail makes it difficult for the authors to know which parts of the paper to focus on for revisions. Therefore, this comment is not grounded and highly unspecific, aligning with the evaluation score 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: It is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.\n\nEVALUATION:\n\n<reasoning>The authors have some idea about which part this comment addresses, as it refers to the attention mechanism in seq2seq MTL. However, they still do not know what specifically needs to be addressed in this part. While they can guess where they mentioned that something is not working, they still cannot identify the specific issues or suggestions for improvement. The comment suggests that the value lies in understanding why the attention mechanism fails and how it can be modified to work effectively. However, without specific guidance or examples, the authors are left without clear direction. Therefore, this comment is weakly grounded and not specific, aligning with the evaluation score 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n\nEVALUATION:\n\n<reasoning>The comment suggests a potential improvement in the system's performance through feature engineering, specifically referencing Uto et al. (2020) as an example. While it does not explicitly mention a specific section of the paper, the authors can infer that it relates to the discussion on performance. The suggestion is specific, as it points to a particular feature set that could be considered. Therefore, the comment is weakly grounded but specific, aligning with the evaluation score 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The paper's title might not accurately reflect the central theme and main contributions of your document. A title is crucial in drawing the interest of readers and offering them a clear understanding of the primary focus of your study, guiding them from the outset. \n\nEVALUATION:\n\n<reasoning>The title is explicitly mentioned, grounding the comment. Despite this, it is under-specific because it doesn\u2019t indicate whether the problem is misleading wording, an inadequate reflection of scope, or the omission of critical terms that would better describe the paper\u2019s content. Hence, this comment is fully grounded and under-specific. Therefore the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics.\n\nEVALUATION:\n\n<reasoning>The comment is fully grounded as it explicitly mentions \"Table 6\" and \"Table 1,\" allowing the authors to accurately identify the parts of the paper being addressed. Additionally, it is specific because it details the issue of alignment between the results in these tables and inquires about the ablation studies of MCT without the adaptive metrics. Therefore, this comment is categorized as fully grounded and specific, aligning with the evaluation score of 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: 3. The experimental results are slightly different from the paper \"Efficient Argument Structure Extraction with Transfer Learning and Active Learning\", and the author needs to give corresponding explanations.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "verifiability_extraction",
            "labels": 0,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should contain claims, opinions, or suggestions that require justification.\n\n\n[CRITERIA]: A statement is considered a claim if it includes any of the following: subjective statements, suggestions or requests for changes, judgments about sections of the paper, deductions or inferred observations, and any statement where evidence or justification is required to support the claim. A statement is considered a normal (non-claim) statement if it meets these criteria: It does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content, it indicates the existence or absence of something without suggesting changes, it makes general statements about the paper that do not express an opinion, it consists of objective, factual statements that do not require verification, it includes requests for clarification or general questions, it presents logical statements or directly inferable information. Scoring rubric is as follows:\n0: If the text consists solely of normal statements.\n1: If the text contains claims, opinions.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n\nEVALUATION:\n\n<reasoning>The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\nEVALUATION:\n\n<reasoning>The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 2>\n\n\n[ANSWER]: - How dependent on the specific parametrization of the MLP (or NN more general) is the performance?",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "coherence",
            "labels": 1,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Given a context from a scientific paper, your task is to write a coherent citation sentence that cites the given paper with a specific citation number. Citation number will be also provided along with the context.\n\n\n[CRITERIA]: The paper context supports (entails) the citation sentence. In cases where more than one paper is referenced in the sentence, as long as context in which given citation number fits the paper content, it should be count as entailment as well. In multiple citation cases, the paper does not have to entail whole sentence. Scoring rubric is as follows:\n0: The given context does not align with the context in which the citation number appears in the sentence, or with the overall meaning of the sentence itself.\n1: The given context is compatible with the context in which the citation number appears in the sentence.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nCONTEXT: Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracy based on the architecture. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 20\u00d7 as sample efficient as Regularized Evolution on the NASBench-101 benchmark and can compete on ImageNet with more complex approaches based on weight sharing, such as ProxylessNAS.\nThe original Neural Architecture Search (NAS) methods have resulted in improved accuracy but they came at a high computational cost [27, 20, 19] . Recent advances have reduced this cost significantly [15, 9, 26, 4, 18, 5, 2, 17, 24, 23, 3] , but many of them require nontrivial specialized implementations. For example, weight sharing introduces additional complexity into the search process, and must be carefully tuned to get good results.\nWith an infinite compute budget, a naive approach to architecture search would be to sample tens or hundreds of thousands of random architectures, train and evaluate each one, and then select the architectures with the best validation set accuracies for deployment; this is a straightforward application of the ubiquitous random search heuristic. However, the computational requirements of this approach makes it infeasible in practice. For example, to exhaustively train and evaluate each of the 400,000 architectures in the NASBench [25] search space, it would take roughly 25 years of TPU training time. Only a small number of companies and corporate research labs can afford this much compute, and it is far out of reach for most ML practitioners.\nOne way to alleviate this is to identify a small subset of promising models. If we can do this with a reasonably high recall (most models selected are indeed of high quality) then we can train and validate just this limited set of models to reliably select a good one for deployment. To achieve this, the proposed Neural Predictor uses the following steps to perform an architecture search:\n(1) Build a predictor by training N random architectures to obtain N (architecture, validation accuracy) pairs. Use this data to train a regression model.\n(2) Quality prediction using the regression model over a large set of random architectures. Select the K most promising architectures for final validation.\n(3) Final validation of the top K architectures by training them. Then we select the model with the highest validation accuracy to deploy.\nThe workflow is illustrated in Figure 1 . In this setup, the first step is a traditional regression problem where we first generate a dataset of N samples to train on. The second step can be carried out efficiently because evaluating a model using the predictor is cheap. The third step is nothing more than traditional validation where we only evaluate a well curated set of K models. While the method outlined above might seem straightforward, it is very effective:\n The Neural Predictor strongly outperforms random search on NASBench-101. It is also about 22.83 times more sample-efficient than Regularized Evolution, the best performing method in the NASBench-101 paper.\n The Neural Predictor can easily handle different search spaces. In addition to NASBench-101, we evaluated it on the ProxylessNAS [4] search space and found that the predicted architecture is as accurate as Proxyless-NAS and clearly better than random search.\n The architecture selection process uses two of the most ubiquitous tools from the ML toolbox: random sampling and supervised learning. In contrast, many existing NAS approaches rely on reinforcement learning, weight sharing, or Bayesian optimization.\n The most computationally intensive components of the proposed method (training N models in step 1 and K models in step 3) are highly parallelizable when sufficient computation resources are available.\n\nCITATION NUMBER: 10\n\nANSWER: Additionally, simple predictors that regress on features extracted from architectures, such as textual encoding schemes or direct structural descriptions, have been found effective for performance approximation, enabling significant sample efficiency improvements [9][10].\n\nEVALUATION:\n\n<reasoning>The, context discusses the effectiveness of a Neural Predictor method that uses regression modeling to predict validation accuracy based on architecture features, achieving significant sample efficiency improvements over traditional approaches. The citation sentence aligns with the context in pointing out the effectiveness of simple predictors for performance approximation, which correlates with the regression steps described in the Neural Predictor method. Therefore, the citation sentence is entailed by the context and follows from it. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nCONTEXT: We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the boring output issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.\nThe task of open-domain dialogue generation is an area of active development, with neural sequence-to-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016) . Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016 ) (ML decoding), or sample from the resulting distribution (Serban et al., 2016) .\nA notorious issue with ML decoding is that this tends to generate short, boring responses to a wide range of inputs, such as \"I don't know\". These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016) . In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007) . To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.\nFurther, natural dialogue is not deterministic; for example, the replies to \"What's your name and where do you come from?\" will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person's responses to certain questions.\nRecently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly. These models have the benefit that, at generation time, we can sample a response from the distribution by first sampling an assignment of the latent variables, and then decoding deterministically. In this way, we introduce stochasticity without resorting to sampling from the decoder, which can lead to incoherent output -see Table 1 for examples.\nIn this paper, we present a latent variable model for one-shot dialogue response, and investigate what kinds of diversity the latent variables capture. Our experiments show that our model has higher lexical as well as sentential diversity than baseline models. We also show that our model generates more acceptable diverse output than sampling from a deterministic decoder. We end by noting that all three methods proposed above to combat the 'maximum likelihood response' are ways of decreasing the probability of the generated output, and report some preliminary results for how response probability interacts with grammaticality and interestingness.\n\nCITATION NUMBER: 11\n\nANSWER: Natural Language Inference has emerged as an essential tool for modeling semantic relationships, with applications extending from textual entailment assessments [11,12] to dialogue consistency verification [17].\n\nEVALUATION:\n\n<reasoning>The citation sentence discusses Natural Language Inference (NLI) and its applications, which is not directly related or entailed by the presented context on dialogue generation models and their improvements. The context does not specifically mention or address NLI or its use in dialogue consistency verification. As such, the hypothesis cannot be inferred or entailed from the given context. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 2>\n\n\n[CONTEXT]: Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.\nBroad-coverage knowledge bases such as Freebase (Bollacker et al., 2008 ) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013) . For example, as of May 2015, Freebase has an entity Tad Lincoln (Abraham Lincoln's son), but does not have his ethnicity. An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015) . In the example, we would hope to infer Tad's ethnicity from the ethnicity of his parents. However, what is missing from these vector space models is the original strength of knowledge bases: the ability to support compositional queries (Ullman, 1985) . For example, we might ask what the ethnicity of Abraham Lincoln's daughter would be. This can be formulated as a path query on the knowledge graph, and we would like a method that can answer this efficiently, while generalizing over missing facts and even missing or hypothetical entities (Abraham Lincoln did not in fact have a daughter).\nIn this paper, we present a scheme to answer path queries on knowledge bases by \"compositionalizing\" a broad class of vector space models that have been used for knowledge base completion (see Figure 1) . At a high level, we interpret the base vector space model as implementing a soft edge traversal operator. This operator can then be recursively applied to predict paths. Our interpretation suggests a new compositional training objective that encourages better modeling of paths. Our technique is applicable to a broad class of composable models that includes the bilinear model (Nickel et al., 2011) and TransE (Bordes et al., 2013) .\nWe have two key empirical findings: First, we show that compositional training enables us to answer path queries up to at least length 5 by substantially reducing cascading errors present in the base vector space model. Second, we find that somewhat surprisingly, compositional training even improves upon state-of-the-art performance for knowledge base completion, which is a special case of answering unit length path queries. Therefore, compositional training can also be seen as a new form of structural regularization for existing models.\n\nCITATION NUMBER: 1\n\n\n[ANSWER]: To answer multi-hop FOL queries, path-based methods [3], [4], [5] start from anchor entities and require [1] the intermediate entities on the path, which leads to exponential computation cost."
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "positioning_type",
            "labels": 0,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Your task is to write related work section that contributions and/or positioning of the paper is provided in the final paragraph as a summary.\n\n\n[CRITERIA]: Each paragraph (except final paragraph) should focus on summarizing and comparing prior research etc., without mentioning the current paper\u2019s contributions or its position in the literature. Then, the final paragraph at the end of the related work section should the current paper\u2019s contributions and its positioning. The final paragraph should address aspects mentioned in previous paragraphs. Scoring rubric is as follows:\n0: The answer provide contributions in each paragraph or final paragraph fails to address the points in previous paragraphs or do not mention contributions-positioning at all.\n1: Only the final paragraph emphasizes the contributions-positioning by addressing the points in previous paragraphs.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: Previous studies on TDMR (Task-Dataset-Metric-Result) extraction primarily rely on data sources such as NLP-progress or PapersWithCode. While these sources are popular, they lack rigorous quality control, including standardized TDM entity representation and complete publication coverage. Some efforts, such as [1] and [2], go further by extracting TDM triples and result values, and normalizing them for leaderboard construction. However, these approaches are constrained by their reliance on a closed domain, requiring extracted triples to match a pre-defined TDM taxonomy. Other studies, such as [3] and [4], only extract TDM triples without result values, thereby forgoing leaderboard construction. Meanwhile, [6] focuses on extracting result values, but only in the context of pre-defined TDM triples, making it similar in limitation to [1]. Consequently, these methods are not adaptable to the dynamic nature of emerging benchmarks, where new tasks, datasets, and metrics are frequently introduced.\nIn a parallel research thread, scientific leaderboards have been treated as a form of scientific knowledge graphs composed of Task, Dataset, and Metric entities. Information extraction in this context has been widely studied, as seen in works such as [5, 7, 8, 9, 10]. These studies typically focus on linking scientific content to structured entities but often assume that the schema or triples are known in advance.\nIn contrast to prior work, our approach addresses several critical limitations. We construct our dataset manually from publications to ensure comprehensive TDMR annotations, rather than relying on incomplete or unstandardized third-party sources. We normalize TDMR tuples for accurate leaderboard construction and propose novel experimental settings that reflect realistic, dynamic scenarios, including cold-start conditions without any pre-defined TDM taxonomy. By simultaneously constructing a scientific knowledge graph and linking publications to it in an open-domain setting, our method is inherently adaptable to the evolving research landscape.\n\nEVALUATION:\n\n<reasoning>The related work draft does not mention the main paper's contributions and its position in each individual paragraph; instead, it waits until the final paragraph to outline the specific contributions of the study. The contributions are clearly summarized and detailed only in the final paragraph. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios.\nPart of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nEVALUATION:\n\n<reasoning>Contributions and position of the paper in the literature are mentioned within each paragraph rather than being summarized at the end. The first paragraph discusses previous work and their limitations, then explicitly states what the current work does differently, listing three specific contributions. The second paragraph again discusses previous studies but also explains how the current work differs by constructing a knowledge graph without assuming predefined triples and linking papers to nodes. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement.\nIn addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nEVALUATION:\n\n<reasoning>The related work section draft provided does not explicitly mention or discuss the main paper's contributions or its position. Each paragraph focuses on summarizing existing research efforts and methodologies without indicating how the current paper builds upon or differs from these works. Additionally, the final paragraph does not serve as a summary of the main paper's contributions; instead, it continues to discuss other related works without tying them back to the current study's advancements. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 3>\n\n\n[ANSWER]: Classic reinforcement\u2011learning formulations assume that a reward function is given a priori, yet in real\u2011world deployments specifying such a function is notoriously difficult.  Prior work therefore explores how agents can acquire reward information from humans, either by asking for direct scalar feedback or by observing other signals.  Our contribution builds on this line by treating natural language as a rich, structured source of reward information and by formally analysing when different linguistic forms are most useful for the learner.\n\nWhen the desired behavior is observable but the underlying reward is hidden, inverse reinforcement learning (IRL) has been the dominant approach.  Existing IRL methods infer a reward that rationalizes observed actions, but they suffer from fundamental identifiability issues and often require additional queries to disambiguate among many compatible rewards.  We extend this tradition by showing how a cooperative speaker can deliberately choose utterances that reduce ambiguity, and we provide a formal characterization of the optimal trade\u2011off between instruction\u2011type and description\u2011type language under varying autonomy horizons.\n\nA growing body of work has examined language\u2011based instruction as a means of shaping agent behaviour.  These studies typically treat instructions as cues about specific actions or goals and use them either to bias policy learning or to recover latent reward preferences.  Our paper situates these approaches within a unified framework that predicts when a human will prefer instructions over more abstract language.  Specifically, we prove that low\u2011autonomy agents benefit most from short, action\u2011oriented utterances, whereas high\u2011autonomy agents profit from longer\u2011horizon, reward\u2011descriptive statements, thereby offering a principled guide for designing instruction\u2011based interfaces.\n\nLearning from descriptive language\u2014textual or free\u2011form explanations of reward structure\u2014has received comparatively less attention because of the grounding challenge it poses.  Existing methods either assume a fixed mapping between words and features or focus on extracting task\u2011relevant concepts from large corpora.  By assuming known groundings, we are able to isolate the informational value of descriptions and to quantify their advantage in settings where the learner must act on novel states.  This theoretical analysis complements empirical work on description\u2011driven teaching and clarifies the conditions under which such language can be safely leveraged.\n\nRecent advances in pragmatic reasoning, particularly the Rational Speech Act (RSA) framework and inverse reward design (IRD), treat human data as intentionally informative rather than merely observational.  These approaches have been applied to reference games, captioning, and instruction following, but they have not been extended to the full spectrum of natural\u2011language reward communication.  We unify RSA\u2011style pragmatic inference with IRD and adapt it to both instructions and descriptions, yielding a pragmatic listener that jointly infers the speaker\u2019s horizon and reward function.  This joint inference mitigates the model\u2011mis\u2011specification vulnerabilities identified in earlier pragmatic learning work.\n\nFinally, while many prior studies evaluate language\u2011based learning in simulation or with synthetic speakers, we validate our models through a behavioral experiment with human participants.  We demonstrate that our speaker model accurately predicts human utterance choices and that the pragmatic listener reliably recovers the underlying reward functions.  Moreover, we show that integrating this social inference with conventional reinforcement learning accelerates learning and reduces regret.  Together, these contributions advance the field from agents that merely obey language to agents that actively learn from it, positioning our work at the intersection of reward learning, linguistic pragmatics, and value alignment."
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "helpfulness",
            "labels": 4,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should be useful for the authors to help improving the paper.\n\n\n[CRITERIA]: A helpful review should be actionable, grounded on a specific part of the paper, provide justification or evidence to its claims. Scoring rubric is as follows:\n1: The comment fails to identify meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\n2: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\n3: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors gain some insights, the feedback does not fully address their needs for improving the draft.\n4: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\n5: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: Lack of novelty: Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\nEVALUATION:\n\n<reasoning>This comment simply recaps a section already addressed in the paper and does not offer constructive guidance or specific suggestions on overcoming the stated issue of lack of novelty. Without actionable advice or an indication of how to differentiate or enhance the work, the comment is not useful to the authors and does not contribute to the paper's improvement, aligning with the evaluation score of 1: The comment is not helpful at all.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\n\nEVALUATION:\n\n<reasoning>The comment highlights a potential issue with the way the probabilistic connection is presented in the paper, indicating it might lack formalism and appear purely motivational. While this could be valuable for the authors, the feedback is not specific enough to be fully actionable. The suggestion to either \"cement this connection more formally\" or \"adjust the language to clarify\" is a start, but it lacks specific guidance or examples that would make it more useful. Therefore, the authors are left with only a vague direction for improvement, making this feedback barely helpful. Therefore, the evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate.\n\nEVALUATION:\n\n<reasoning>The comment identifies a potential issue with the specificity of the proposed methodology, questioning whether it is limited to bimanual manipulation or if it could be generalized to robotic manipulation. This is a relevant point that could help the authors clarify the scope and applicability of their methodology. However, the comment lacks depth and does not provide specific suggestions or guidance on how the authors might address this issue. It raises an important question but does not offer actionable steps for improvement, making it somewhat helpful. Therefore, the comment is evaluated as somewhat helpful, consistent with the evaluation score 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs.\n\nEVALUATION:\n\n<reasoning>The review comment provides a clear and actionable suggestion for improvement by recommending the authors perform ablation experiments. It specifies the need to compare the proposed method with other methods, such as TubeR, focusing on the number of learnable parameters and GFLOPs. This feedback is direct and offers a concrete step for the authors to enhance their paper, making it mostly helpful. However, it could be further improved by explaining why these comparisons are important or how they might impact the paper's conclusions, which would elevate it to a highly helpful comment. Therefore, it aligns with a score of 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\n\nEVALUATION:\n\n<reasoning>The comment is highly helpful as it provides a clear and specific suggestion for an additional experiment that could significantly enhance the paper. By proposing an experiment where the image is occluded, the reviewer not only identifies a potential area for improvement but also explains the rationale behind it\u2014simulating irregularities in data and testing the model's inference capacity. This feedback is actionable and constructive, offering the authors a concrete way to strengthen their work. The expectation that these experiments be included unless justified otherwise further emphasizes the importance of this suggestion, making it a valuable contribution to the paper's development.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: 2. It is unclear whether some of the baseline results reported are author reproductions or original reported numbers, since the numbers do not always seem to match what is reported in the original papers. Since the main point of the work is improvement in empirical performance, this needs to be made more clear. [1] Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models, Findings of ACL 2021 1. Why is the font in tables so small? Also, spacing between sections seems to have been significantly reduced compared to the ACL template.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "actionability",
            "labels": 3,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should be actionable. Those actions should be clearly identifiable and concrete.\n\n\n[CRITERIA]: Explicit actions or suggestions are direct or apparent. Authors can directly identify modifications they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action. However, implicit actions need to be inferred from the comment. This includes missing parts that need to be added. Authors can deduce what needs to be done after reading the comment. For concrete actions, the authors know exactly what needs to be done and how to apply the action. However, for vague actions the authors still don\u2019t know how to carry out this action. Scoring rubric is as follows:\n1: The comment lacks meaningful information to help authors improve the paper. Authors do not know what they should do after reading the comment.\n2: The comment includes an implicitly stated action or an action that can be inferred. However, the action itself is vague and lacks detail on how to apply it.\n3: The comment explicitly states an action but is vague on how to execute it.\n4: The comment implicitly states an action but concretely states how to implement the inferred action.\n5: The comment contains an explicit action and concrete details on how to implement it. Authors know exactly how to apply it.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: The hGRU architecture seems pretty ad-hoc and not very well motivated.\n\nEVALUATION:\n\n<reasoning>The review comment, \"The hGRU architecture seems pretty ad-hoc and not very well motivated,\" lacks specificity and actionable guidance for the authors. While it expresses a concern about the hGRU architecture being \"ad-hoc\" and \"not very well motivated,\" it does not provide any detailed explanation or examples of why the reviewer perceives it this way. Without specific points or suggestions, the authors are left without a clear understanding of what aspects of the architecture need further clarification or improvement. hence, this comment is not actionable at all. Therefore the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n\nEVALUATION:\n\n<reasoning>The comment highlights the need for the paper to delve deeper into the theory proofs and demonstrate the convergence properties of the proposed algorithm. However, it does not specify which aspects of the theory proofs require further exploration or how the authors should approach this task. The lack of concrete recommendations regarding the convergence properties leaves the authors uncertain about how to fulfill this request effectively. Due to its vague and indirect nature, this comment is barely actionable. Therefore the evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: A number of claims from this paper would benefit from more in-depth analysis.\n\nEVALUATION:\n\n<reasoning>The comment points out that certain claims require more in-depth analysis but does not clarify which claims need further scrutiny. As a result, the authors may not know where to focus their efforts, leading to potential misinterpretation of the feedback. Since the suggested action is direct but still lacks the necessary details for precise implementation, this comment is somewhat actionable. Therefore the evaluation score should be 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8.\n\nEVALUATION:\n\n<reasoning>The reviewer identifies specific actions that the authors did not take, such as not comparing their work to Campos et al. (2020) and other domain adaptation methods cited in Section 8. This serves as an implicit suggestion that these comparisons should be implemented. However, rather than explicitly stating what the authors should do, the comment leaves room for interpretation. Because the required action is clear but not directly stated, the comment is mostly actionable. Therefore the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\n\nEVALUATION:\n\n<reasoning>The comment explicitly states that an example of inconsistency should be provided in the abstract, specifically where it mentions \"evaluating with gold answers is inconsistent with human evaluation.\" By directly instructing the authors to include an example, such as how models get ranked differently, it removes any uncertainty about how to proceed. Since the feedback is clear, specific, and directly actionable, the comment is fully actionable. Therefore the evaluation score should be 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: - Same for Sec 2. Causal graph is defined wrongly or incomplete at least; the Markov factorization is not what makes an CBN, since it applies to standard BNs. See definitions of CBN in eg [Pearl, 2009, Definition 1.3.1] , it's about all interventional distributions having the Markov fact. and other constraints.",
                    "role": "user"
                }
            ]
        }
    ],
    "test": [
        {
            "task": "rev_util",
            "aspect": "verifiability_extraction",
            "labels": 1,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should contain claims, opinions, or suggestions that require justification.\n\n\n[CRITERIA]: A statement is considered a claim if it includes any of the following: subjective statements, suggestions or requests for changes, judgments about sections of the paper, deductions or inferred observations, and any statement where evidence or justification is required to support the claim. A statement is considered a normal (non-claim) statement if it meets these criteria: It does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content, it indicates the existence or absence of something without suggesting changes, it makes general statements about the paper that do not express an opinion, it consists of objective, factual statements that do not require verification, it includes requests for clarification or general questions, it presents logical statements or directly inferable information. Scoring rubric is as follows:\n0: If the text consists solely of normal statements.\n1: If the text contains claims, opinions.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n\nEVALUATION:\n\n<reasoning>The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\nEVALUATION:\n\n<reasoning>The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 2>\n\n\n[ANSWER]: - In experiments, the datasets used are Criteo and Avazu. More real-world datas should be considered as important test beds, e.g., iPinYou, etc.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "positioning_type",
            "labels": 1,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Your task is to write related work section for a scientific paper that states the paper's contribution or its position among the literature in each paragraph.\n\n\n[CRITERIA]: In a multiple paragraph related work section, every single paragraph should state the contribution of the paper and/or its position among the literature. Ensure that contribution and/or positioning statements should be aligned with the specific focus of each paragraph. Scoring rubric is as follows:\n0: The answer fails to state contribution of the paper and/or its position among the literature in each related work paragraph.\n1: The answer states contribution of the paper and/or its position among the literature in each related work paragraph.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: Previous studies on TDMR (Task-Dataset-Metric-Result) extraction primarily rely on data sources such as NLP-progress or PapersWithCode. While these sources are popular, they lack rigorous quality control, including standardized TDM entity representation and complete publication coverage. Some efforts, such as [1] and [2], go further by extracting TDM triples and result values, and normalizing them for leaderboard construction. However, these approaches are constrained by their reliance on a closed domain, requiring extracted triples to match a pre-defined TDM taxonomy. Other studies, such as [3] and [4], only extract TDM triples without result values, thereby forgoing leaderboard construction. Meanwhile, [6] focuses on extracting result values, but only in the context of pre-defined TDM triples, making it similar in limitation to [1]. Consequently, these methods are not adaptable to the dynamic nature of emerging benchmarks, where new tasks, datasets, and metrics are frequently introduced.\nIn a parallel research thread, scientific leaderboards have been treated as a form of scientific knowledge graphs composed of Task, Dataset, and Metric entities. Information extraction in this context has been widely studied, as seen in works such as [5, 7, 8, 9, 10]. These studies typically focus on linking scientific content to structured entities but often assume that the schema or triples are known in advance.\nIn contrast to prior work, our approach addresses several critical limitations. We construct our dataset manually from publications to ensure comprehensive TDMR annotations, rather than relying on incomplete or unstandardized third-party sources. We normalize TDMR tuples for accurate leaderboard construction and propose novel experimental settings that reflect realistic, dynamic scenarios, including cold-start conditions without any pre-defined TDM taxonomy. By simultaneously constructing a scientific knowledge graph and linking publications to it in an open-domain setting, our method is inherently adaptable to the evolving research landscape.\n\nEVALUATION:\n\n<reasoning>The related work draft does not mention the main paper's contributions and its position in each individual paragraph; instead, it waits until the final paragraph to outline the specific contributions of the study. The contributions are clearly summarized and detailed only in the final paragraph. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios.\nPart of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nEVALUATION:\n\n<reasoning>Contributions and position of the paper in the literature are mentioned within each paragraph rather than being summarized at the end. The first paragraph discusses previous work and their limitations, then explicitly states what the current work does differently, listing three specific contributions. The second paragraph again discusses previous studies but also explains how the current work differs by constructing a knowledge graph without assuming predefined triples and linking papers to nodes. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement.\nIn addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nEVALUATION:\n\n<reasoning>The related work section draft provided does not explicitly mention or discuss the main paper's contributions or its position. Each paragraph focuses on summarizing existing research efforts and methodologies without indicating how the current paper builds upon or differs from these works. Additionally, the final paragraph does not serve as a summary of the main paper's contributions; instead, it continues to discuss other related works without tying them back to the current study's advancements. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 3>\n\n\n[ANSWER]: Marginal\u2011likelihood approximations have long been a cornerstone of scalable Gaussian\u2011process inference, predating the introduction of pseudo\u2011inputs and the variational\u2011inducing\u2011point framework. Early stochastic treatments of the evidence lower bound (e.g., Hern\u00e1ndez\u2011Lobato & Adams, 2015; Salimbeni & Deisenroth, 2017) can be seen as precursors to the Bayesian perspective adopted in our work. While those methods approximate the ELBO by subsampling data points, they still rely on a fixed set of inducing variables that must be learned jointly with the latent representation. In contrast, the stochastic active\u2011set (SAS) approach we propose replaces learned inducing points with randomly sampled subsets of the training data and derives a stochastic estimator of the log\u2011marginal likelihood that remains unbiased. This positions SAS as a true Bayesian counterpart to earlier stochastic ELBO approximations, offering a simpler optimisation landscape and eliminating the need for explicit inducing\u2011point optimisation.  \n\nActive\u2011set approximations were among the first sparse GP techniques (Smola & Scholkopf, 2001) and have been empirically shown to remain stable even when the subset size is a small fraction of the dataset (Qui\u00f1onero\u2011Candela & Rasmussen, 2005). However, subsequent work reported that na\u00efve random selection of active sets can induce noisy, non\u2011smooth objective fluctuations that hinder gradient\u2011based optimisation, motivating the shift toward continuous pseudo\u2011inputs (Snelson & Ghahramani, 2006). Our contribution directly addresses this limitation: by integrating the active\u2011set selection into a stochastic optimisation loop, SAS treats the subset as a random variable and averages over many realizations, thereby smoothing the training objective without sacrificing the computational benefits of small subsets. Empirically, this yields faster convergence and more reliable training of GP decoders than both fixed\u2011active\u2011set schemes and variational inducing\u2011point methods.  \n\nThe relationship between cross\u2011validation (CV) and Gaussian\u2011process marginal\u2011likelihoods has been explored for model selection (Wahba, 1990; Rasmussen, 2006), and recent theoretical work has shown an exact equivalence between exhaustive CV and the log\u2011marginal likelihood (Fong & Holmes, 2020). Existing GP\u2011LVM extensions that incorporate back\u2011constraints or amortised inference (Damianou et al., 2016; Sun et al., 2022) still rely on deterministic inducing variables and therefore inherit the optimisation challenges associated with them. By leveraging the CV\u2013marginal\u2011likelihood equivalence, our SAS framework obtains a tractable stochastic estimator of the marginal likelihood that naturally accommodates random active\u2011set sampling. This connection enables us to train GP decoders that discover richer latent structures, scale to larger datasets, and outperform both variational auto\u2011encoders and state\u2011of\u2011the\u2011art GP\u2011LVMs that use fixed inducing points."
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "coherence",
            "labels": 1,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Given a context from a scientific paper, your task is to write a coherent citation sentence that cites the given paper with a specific citation number. Citation number will be also provided along with the context.\n\n\n[CRITERIA]: The paper context supports (entails) the citation sentence. In cases where more than one paper is referenced in the sentence, as long as context in which given citation number fits the paper content, it should be count as entailment as well. In multiple citation cases, the paper does not have to entail whole sentence. Scoring rubric is as follows:\n0: The given context does not align with the context in which the citation number appears in the sentence, or with the overall meaning of the sentence itself.\n1: The given context is compatible with the context in which the citation number appears in the sentence.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nCONTEXT: Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracy based on the architecture. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 20\u00d7 as sample efficient as Regularized Evolution on the NASBench-101 benchmark and can compete on ImageNet with more complex approaches based on weight sharing, such as ProxylessNAS.\nThe original Neural Architecture Search (NAS) methods have resulted in improved accuracy but they came at a high computational cost [27, 20, 19] . Recent advances have reduced this cost significantly [15, 9, 26, 4, 18, 5, 2, 17, 24, 23, 3] , but many of them require nontrivial specialized implementations. For example, weight sharing introduces additional complexity into the search process, and must be carefully tuned to get good results.\nWith an infinite compute budget, a naive approach to architecture search would be to sample tens or hundreds of thousands of random architectures, train and evaluate each one, and then select the architectures with the best validation set accuracies for deployment; this is a straightforward application of the ubiquitous random search heuristic. However, the computational requirements of this approach makes it infeasible in practice. For example, to exhaustively train and evaluate each of the 400,000 architectures in the NASBench [25] search space, it would take roughly 25 years of TPU training time. Only a small number of companies and corporate research labs can afford this much compute, and it is far out of reach for most ML practitioners.\nOne way to alleviate this is to identify a small subset of promising models. If we can do this with a reasonably high recall (most models selected are indeed of high quality) then we can train and validate just this limited set of models to reliably select a good one for deployment. To achieve this, the proposed Neural Predictor uses the following steps to perform an architecture search:\n(1) Build a predictor by training N random architectures to obtain N (architecture, validation accuracy) pairs. Use this data to train a regression model.\n(2) Quality prediction using the regression model over a large set of random architectures. Select the K most promising architectures for final validation.\n(3) Final validation of the top K architectures by training them. Then we select the model with the highest validation accuracy to deploy.\nThe workflow is illustrated in Figure 1 . In this setup, the first step is a traditional regression problem where we first generate a dataset of N samples to train on. The second step can be carried out efficiently because evaluating a model using the predictor is cheap. The third step is nothing more than traditional validation where we only evaluate a well curated set of K models. While the method outlined above might seem straightforward, it is very effective:\n The Neural Predictor strongly outperforms random search on NASBench-101. It is also about 22.83 times more sample-efficient than Regularized Evolution, the best performing method in the NASBench-101 paper.\n The Neural Predictor can easily handle different search spaces. In addition to NASBench-101, we evaluated it on the ProxylessNAS [4] search space and found that the predicted architecture is as accurate as Proxyless-NAS and clearly better than random search.\n The architecture selection process uses two of the most ubiquitous tools from the ML toolbox: random sampling and supervised learning. In contrast, many existing NAS approaches rely on reinforcement learning, weight sharing, or Bayesian optimization.\n The most computationally intensive components of the proposed method (training N models in step 1 and K models in step 3) are highly parallelizable when sufficient computation resources are available.\n\nCITATION NUMBER: 10\n\nANSWER: Additionally, simple predictors that regress on features extracted from architectures, such as textual encoding schemes or direct structural descriptions, have been found effective for performance approximation, enabling significant sample efficiency improvements [9][10].\n\nEVALUATION:\n\n<reasoning>The, context discusses the effectiveness of a Neural Predictor method that uses regression modeling to predict validation accuracy based on architecture features, achieving significant sample efficiency improvements over traditional approaches. The citation sentence aligns with the context in pointing out the effectiveness of simple predictors for performance approximation, which correlates with the regression steps described in the Neural Predictor method. Therefore, the citation sentence is entailed by the context and follows from it. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nCONTEXT: We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the boring output issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.\nThe task of open-domain dialogue generation is an area of active development, with neural sequence-to-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016) . Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016 ) (ML decoding), or sample from the resulting distribution (Serban et al., 2016) .\nA notorious issue with ML decoding is that this tends to generate short, boring responses to a wide range of inputs, such as \"I don't know\". These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016) . In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007) . To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.\nFurther, natural dialogue is not deterministic; for example, the replies to \"What's your name and where do you come from?\" will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person's responses to certain questions.\nRecently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly. These models have the benefit that, at generation time, we can sample a response from the distribution by first sampling an assignment of the latent variables, and then decoding deterministically. In this way, we introduce stochasticity without resorting to sampling from the decoder, which can lead to incoherent output -see Table 1 for examples.\nIn this paper, we present a latent variable model for one-shot dialogue response, and investigate what kinds of diversity the latent variables capture. Our experiments show that our model has higher lexical as well as sentential diversity than baseline models. We also show that our model generates more acceptable diverse output than sampling from a deterministic decoder. We end by noting that all three methods proposed above to combat the 'maximum likelihood response' are ways of decreasing the probability of the generated output, and report some preliminary results for how response probability interacts with grammaticality and interestingness.\n\nCITATION NUMBER: 11\n\nANSWER: Natural Language Inference has emerged as an essential tool for modeling semantic relationships, with applications extending from textual entailment assessments [11,12] to dialogue consistency verification [17].\n\nEVALUATION:\n\n<reasoning>The citation sentence discusses Natural Language Inference (NLI) and its applications, which is not directly related or entailed by the presented context on dialogue generation models and their improvements. The context does not specifically mention or address NLI or its use in dialogue consistency verification. As such, the hypothesis cannot be inferred or entailed from the given context. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 2>\n\n\n[CONTEXT]: The thud of a bouncing ball, the onset of speech as lips open -- when visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that the visual and audio components of a video signal should be modeled jointly using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the source of sound in a video; (b) audio-visual action recognition; and (c) on/off-screen audio source separation, e.g. removing the off-screen translator's voice from a foreign official's speech. Code, models, and video results are available on our webpage: this http URL\nAs humans, we experience our world through a number of simultaneous sensory streams. When we bite into an apple, not only do we taste it, but -as Smith and Gasser [1] point out -we also hear it crunch, see its red skin, and feel the coolness of its core. The coincidence of sensations gives us strong evidence that they were generated by a common, underlying event [2] , since it is unlikely that they co-occurred across multiple modalities merely by chance. These cross-modal, temporal co-occurrences therefore provide a useful learning signal: a model that is trained to detect them ought to discover multimodal structures that are useful for other tasks. In much of traditional computer vision research, however, we have been avoiding the use of other, non-visual modalities, arguably making the perception problem harder, not easier.\nIn this paper, we learn a temporal, multisensory representation that fuses the visual and audio components of a video signal. We propose to train this model without using any manually labeled data. That is, rather than explicitly telling the model that, e.g., it should associate moving lips with speech or a thud with a bouncing ball, we have it discover these audio-visual associations through self-supervised training [3] . Specifically, we train a neural network on a \"pretext\" task of detecting misalignment between audio and visual streams in synthetically-shifted videos. The network observes raw audio and video streams -some of which are aligned, and some that have been randomly shifted by a few seconds -and we task it with distinguishing between the two. This turns out to be a challenging training task that forces the network to fuse visual motion with audio information and, in the process, learn a useful audio-visual feature representation.\nWe demonstrate the usefulness of our multisensory representation in three audiovisual applications: (a) sound source localization, (b) audio-visual action recognition; and (c) on/off-screen sound source separation. Figure 1 shows examples of these applications. In Fig. 1(a) , we visualize the sources of sound in a video using our network's learned attention map, i.e. the impact of an axe, the opening of a mouth, and moving hands of a musician. In Fig. 1(b) , we show an application of our learned features to audio-visual action recognition, i.e. classifying a video of a chef chopping an onion. In Fig. 1(c) , we demonstrate our novel on/off-screen sound source separation model's ability to separate the speakers' voices by visually masking them from the video. The main contributions of this paper are: 1) learning a general video representation that fuses audio and visual information; 2) evaluating the usefulness of this representation qualitatively (by sound source visualization) and quantitatively (on an action recognition task); and 3) proposing a novel video-conditional source separation method that uses our representation to separate on-and off-screen sounds, and is the first method to work successfully on real-world video footage, e.g. television broadcasts. Our feature representation, as well as code and models for all applications are available online.\n\nCITATION NUMBER: 50\n\n\n[ANSWER]: Video-audio joint analysis [49], [50], [51] utilizes modality consistency to learn representations."
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "grounding_specificity",
            "labels": 5,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should refer to a specific part of the paper and clearly identify the issue with that part.\n\n\n[CRITERIA]: For fully grounded comment, the author can accurately pinpoint the section, table, figure, or unique aspect being addressed. For weak grounded comment, the author can make an educated guess but cannot precisely identify the referenced part. For specificity, the comment should detail what is wrong or missing in the referenced part. If external work is mentioned, it should also provide specific examples. Scoring rubric is as follows:\n1: The comment is not grounded at all. It does not identify a specific area in the paper. The comment is highly unspecific.\n2: The authors cannot confidently determine which part the comment addresses. Further, the comment does not specify what needs to be addressed in this part.\n3: The authors cannot confidently determine which part the comment addresses. However, the comment clearly specifies what needs to be addressed in this part.\n4: The comment explicitly mentions which part of the paper it addresses, or it should be obvious to the authors. However, this comment does not specify what needs to be addressed in this part.\n5: The comment explicitly mentions which part of the paper it addresses, and it is obvious to the authors. The comment specifies what needs to be addressed in this part.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: The writing should be improved. Some points in the paper is unclear to me.\n\nEVALUATION:\n\n<reasoning>The comment is not grounded as it does not identify any specific part of the paper that needs improvement. It generally mentions that the writing should be improved and that some points are unclear, but it does not specify which points or sections are problematic. This lack of detail makes it difficult for the authors to know which parts of the paper to focus on for revisions. Therefore, this comment is not grounded and highly unspecific, aligning with the evaluation score 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: It is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.\n\nEVALUATION:\n\n<reasoning>The authors have some idea about which part this comment addresses, as it refers to the attention mechanism in seq2seq MTL. However, they still do not know what specifically needs to be addressed in this part. While they can guess where they mentioned that something is not working, they still cannot identify the specific issues or suggestions for improvement. The comment suggests that the value lies in understanding why the attention mechanism fails and how it can be modified to work effectively. However, without specific guidance or examples, the authors are left without clear direction. Therefore, this comment is weakly grounded and not specific, aligning with the evaluation score 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n\nEVALUATION:\n\n<reasoning>The comment suggests a potential improvement in the system's performance through feature engineering, specifically referencing Uto et al. (2020) as an example. While it does not explicitly mention a specific section of the paper, the authors can infer that it relates to the discussion on performance. The suggestion is specific, as it points to a particular feature set that could be considered. Therefore, the comment is weakly grounded but specific, aligning with the evaluation score 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The paper's title might not accurately reflect the central theme and main contributions of your document. A title is crucial in drawing the interest of readers and offering them a clear understanding of the primary focus of your study, guiding them from the outset. \n\nEVALUATION:\n\n<reasoning>The title is explicitly mentioned, grounding the comment. Despite this, it is under-specific because it doesn\u2019t indicate whether the problem is misleading wording, an inadequate reflection of scope, or the omission of critical terms that would better describe the paper\u2019s content. Hence, this comment is fully grounded and under-specific. Therefore the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics.\n\nEVALUATION:\n\n<reasoning>The comment is fully grounded as it explicitly mentions \"Table 6\" and \"Table 1,\" allowing the authors to accurately identify the parts of the paper being addressed. Additionally, it is specific because it details the issue of alignment between the results in these tables and inquires about the ablation studies of MCT without the adaptive metrics. Therefore, this comment is categorized as fully grounded and specific, aligning with the evaluation score of 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: - fetched toxic sentences for rewriting (line 215), - toxicity classifier (line 218) - annotator bias These choices introduce biases into the datasets (unavoidable). The paper would benefit from attempting to quantify how useful a model trained on ParaDetox is in a more general setting. e.g. performance on other (non-parallel) toxicity datasets.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rw_gen",
            "aspect": "positioning_check",
            "labels": 0,
            "score_sets": [
                0,
                1
            ],
            "prompt": [
                {
                    "role": "system",
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else."
                },
                {
                    "role": "user",
                    "content": "[QUERY]: Your task is to write a related work section paragraph for a scientific paper that states the paper's contribution or its position among the literature in each paragraph.\n\n\n[CRITERIA]: The generated paragraph should explicitly or implicitly mention the main paper's contribution or position among existing literature. Ensure that contribution and/or positioning statements should be aligned with the specific focus of the paragraph. Scoring rubric is as follows:\n0: The answer paragraph fails to explicitly or implicitly mention the main paper's contribution or position among existing literature.\n1: The answer paragraph explicitly or implicitly mention the main paper's contribution or position among existing literature.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios. Part of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nEVALUATION:\n\n<reasoning>The draft states the main paper's contribution and how it differs from existing literature. It outlines the limitations of previous studies and then explicitly states how the current work addresses these issues through specific contributions, such as manual dataset construction and handling cold start scenarios without pre-defined TDM triples. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement. In addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\n\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nEVALUATION:\n\n<reasoning>The draft provides a comprehensive overview of related work but does not explicitly or implicitly mention the main paper's own contribution or how it differs from the existing literature. It talks about the efforts made but doesn't bridge into how this work builds upon them or addresses gaps they left. It focuses solely on summarizing prior work without establishing the unique position or advancement made by the current study. Therefore, the evaluation score should be 0.</reasoning>\n\n<score>0</score>\n\n<END OF EXAMPLE 2>\n\n\n[ANSWER]: Averaging of model parameters has a long history in maximum\u2011likelihood estimation. Uniform averaging and exponential moving averaging (EMA) of parameters have been employed in a variety of settings to reduce variance and improve generalization\u202f[7\u201310]. While these techniques are well understood for convex optimization problems, generative adversarial networks correspond to saddle\u2011point (minimax) problems, and the theoretical guarantees for averaging in the convex case do not directly carry over to the adversarial setting."
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "verifiability",
            "labels": 1,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The claims in your comment should be justified or proved by providing logical reasoning, using common sense, or referencing external sources.\n\n\n[CRITERIA]: Claim justification-verification can be done either by logical reasoning supporting the claim, common sense knowledge in the field verifying the claim (e.g., referencing established practices or standards), or external references substantiating the claim. Scoring rubric is as follows:\n1: The comment contains a claim without any supporting evidence or justification.\n2: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\n3: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\n4: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\n5: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through: - Clear and precise reasoning or explanation. - Specific and relevant references to external works or data. - Logical and unassailable common-sense arguments.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: A number of claims from this paper would benefit from more in-depth analysis.\n\nEVALUATION:\n\n<reasoning>The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n\nEVALUATION:\n\n<reasoning>The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable. Therefore, evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n\nEVALUATION:\n\n<reasoning>The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at the evaluation score 3.\"</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n\nEVALUATION:\n\n<reasoning>The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n\nEVALUATION:\n\n<reasoning>The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is fully verifiable and should get the evaluation score 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: 5. KDSM essentially is matching and retrival. The global matching matrix will cause K-k invalid text prompts, which would waste computation.",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "actionability",
            "labels": 1,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should be actionable. Those actions should be clearly identifiable and concrete.\n\n\n[CRITERIA]: Explicit actions or suggestions are direct or apparent. Authors can directly identify modifications they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action. However, implicit actions need to be inferred from the comment. This includes missing parts that need to be added. Authors can deduce what needs to be done after reading the comment. For concrete actions, the authors know exactly what needs to be done and how to apply the action. However, for vague actions the authors still don\u2019t know how to carry out this action. Scoring rubric is as follows:\n1: The comment lacks meaningful information to help authors improve the paper. Authors do not know what they should do after reading the comment.\n2: The comment includes an implicitly stated action or an action that can be inferred. However, the action itself is vague and lacks detail on how to apply it.\n3: The comment explicitly states an action but is vague on how to execute it.\n4: The comment implicitly states an action but concretely states how to implement the inferred action.\n5: The comment contains an explicit action and concrete details on how to implement it. Authors know exactly how to apply it.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: The hGRU architecture seems pretty ad-hoc and not very well motivated.\n\nEVALUATION:\n\n<reasoning>The review comment, \"The hGRU architecture seems pretty ad-hoc and not very well motivated,\" lacks specificity and actionable guidance for the authors. While it expresses a concern about the hGRU architecture being \"ad-hoc\" and \"not very well motivated,\" it does not provide any detailed explanation or examples of why the reviewer perceives it this way. Without specific points or suggestions, the authors are left without a clear understanding of what aspects of the architecture need further clarification or improvement. hence, this comment is not actionable at all. Therefore the evaluation score should be 1.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n\nEVALUATION:\n\n<reasoning>The comment highlights the need for the paper to delve deeper into the theory proofs and demonstrate the convergence properties of the proposed algorithm. However, it does not specify which aspects of the theory proofs require further exploration or how the authors should approach this task. The lack of concrete recommendations regarding the convergence properties leaves the authors uncertain about how to fulfill this request effectively. Due to its vague and indirect nature, this comment is barely actionable. Therefore the evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: A number of claims from this paper would benefit from more in-depth analysis.\n\nEVALUATION:\n\n<reasoning>The comment points out that certain claims require more in-depth analysis but does not clarify which claims need further scrutiny. As a result, the authors may not know where to focus their efforts, leading to potential misinterpretation of the feedback. Since the suggested action is direct but still lacks the necessary details for precise implementation, this comment is somewhat actionable. Therefore the evaluation score should be 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8.\n\nEVALUATION:\n\n<reasoning>The reviewer identifies specific actions that the authors did not take, such as not comparing their work to Campos et al. (2020) and other domain adaptation methods cited in Section 8. This serves as an implicit suggestion that these comparisons should be implemented. However, rather than explicitly stating what the authors should do, the comment leaves room for interpretation. Because the required action is clear but not directly stated, the comment is mostly actionable. Therefore the evaluation score should be 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\n\nEVALUATION:\n\n<reasoning>The comment explicitly states that an example of inconsistency should be provided in the abstract, specifically where it mentions \"evaluating with gold answers is inconsistent with human evaluation.\" By directly instructing the authors to include an example, such as how models get ranked differently, it removes any uncertainty about how to proceed. Since the feedback is clear, specific, and directly actionable, the comment is fully actionable. Therefore the evaluation score should be 5.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: * The algorithm still requires ground-truth annotations of the datasets (even if it doesn't require a CoT rationale).",
                    "role": "user"
                }
            ]
        },
        {
            "task": "rev_util",
            "aspect": "helpfulness",
            "labels": 3,
            "score_sets": [
                1,
                2,
                3,
                4,
                5
            ],
            "prompt": [
                {
                    "content": "You are an evaluator of expert-domain scientific writing. You will get a query-answer pair along with criteria explaining the specific evaluation aspect and the scoring rubric. You should evaluate whether the answer satisfy the query based on the given criteria. In addition, examples demonstrating how the evaluation should be performed will be provided. First output your reasoning enclosed between <reasoning> and </reasoning>. Then, output your score enclosed between <score> and </score>. Inside <score> provide only the numeric score and nothing else.",
                    "role": "system"
                },
                {
                    "content": "[QUERY]: Your task is to write a review comment for a scientific paper. The comment should be useful for the authors to help improving the paper.\n\n\n[CRITERIA]: A helpful review should be actionable, grounded on a specific part of the paper, provide justification or evidence to its claims. Scoring rubric is as follows:\n1: The comment fails to identify meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\n2: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\n3: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors gain some insights, the feedback does not fully address their needs for improving the draft.\n4: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\n5: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\n\n\n[EXAMPLES]:\n\n<START OF EXAMPLE 1>\n\nANSWER: Lack of novelty: Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\nEVALUATION:\n\n<reasoning>This comment simply recaps a section already addressed in the paper and does not offer constructive guidance or specific suggestions on overcoming the stated issue of lack of novelty. Without actionable advice or an indication of how to differentiate or enhance the work, the comment is not useful to the authors and does not contribute to the paper's improvement, aligning with the evaluation score of 1: The comment is not helpful at all.</reasoning>\n\n<score>1</score>\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nANSWER: I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\n\nEVALUATION:\n\n<reasoning>The comment highlights a potential issue with the way the probabilistic connection is presented in the paper, indicating it might lack formalism and appear purely motivational. While this could be valuable for the authors, the feedback is not specific enough to be fully actionable. The suggestion to either \"cement this connection more formally\" or \"adjust the language to clarify\" is a start, but it lacks specific guidance or examples that would make it more useful. Therefore, the authors are left with only a vague direction for improvement, making this feedback barely helpful. Therefore, the evaluation score should be 2.</reasoning>\n\n<score>2</score>\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nANSWER: It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate.\n\nEVALUATION:\n\n<reasoning>The comment identifies a potential issue with the specificity of the proposed methodology, questioning whether it is limited to bimanual manipulation or if it could be generalized to robotic manipulation. This is a relevant point that could help the authors clarify the scope and applicability of their methodology. However, the comment lacks depth and does not provide specific suggestions or guidance on how the authors might address this issue. It raises an important question but does not offer actionable steps for improvement, making it somewhat helpful. Therefore, the comment is evaluated as somewhat helpful, consistent with the evaluation score 3.</reasoning>\n\n<score>3</score>\n\n<END OF EXAMPLE 3>\n\n\n<START OF EXAMPLE 4>\n\nANSWER: The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs.\n\nEVALUATION:\n\n<reasoning>The review comment provides a clear and actionable suggestion for improvement by recommending the authors perform ablation experiments. It specifies the need to compare the proposed method with other methods, such as TubeR, focusing on the number of learnable parameters and GFLOPs. This feedback is direct and offers a concrete step for the authors to enhance their paper, making it mostly helpful. However, it could be further improved by explaining why these comparisons are important or how they might impact the paper's conclusions, which would elevate it to a highly helpful comment. Therefore, it aligns with a score of 4.</reasoning>\n\n<score>4</score>\n\n<END OF EXAMPLE 4>\n\n\n<START OF EXAMPLE 5>\n\nANSWER: Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\n\nEVALUATION:\n\n<reasoning>The comment is highly helpful as it provides a clear and specific suggestion for an additional experiment that could significantly enhance the paper. By proposing an experiment where the image is occluded, the reviewer not only identifies a potential area for improvement but also explains the rationale behind it\u2014simulating irregularities in data and testing the model's inference capacity. This feedback is actionable and constructive, offering the authors a concrete way to strengthen their work. The expectation that these experiments be included unless justified otherwise further emphasizes the importance of this suggestion, making it a valuable contribution to the paper's development.</reasoning>\n\n<score>5</score>\n\n<END OF EXAMPLE 5>\n\n\n[ANSWER]: \u2022 Is Lemma 1 a well-known result for any DAG? The Jacobian can be transformed to an upper or lower triangular matrix with unit diagonals by permuting the nodes and hence the determinant is trivially 1. I might have missed something here.",
                    "role": "user"
                }
            ]
        }
    ]
}